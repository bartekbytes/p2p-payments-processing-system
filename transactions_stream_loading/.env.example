APP_NAME=transactions_stream_loading
SOURCE_SYSTEM_ID=2 # transactions_stream_loading system from Master Data

# Number of messages to insert into the DB in a batch operation
TRANSACTION_DB_BATCH_SIZE=10

# Maximum time (in seconds) before flushing the current batch to the DB, 
# even if the batch is not full
KAFKA_CONSUMER_TRANSACTION_FLUSH_INTERVAL_SECONDS=60

# Number of retry attempts when connecting to the Kafka broker
KAFKA_CONSUMER_RETRIES=10

# Initial delay (in seconds) before the first retry attempt (used in exponential backoff)
KAFKA_CONSUMER_INITIAL_DELAY=5


# common-logger section
LOG_LEVEL=INFO

LOG_TO_CONSOLE=true
LOG_JSON_CONSOLE=false

LOG_TO_ELK=false

LOG_TO_FILE=false
LOG_FILE_PATH=logs/transactions_stream_loading.log
LOG_JSON_FILE=false

# Overriding DB section, as this project is dockerized
# For localhost run:
#DB_CONNECTION_URL="postgresql://postgres:postgresql@localhost:5432/pps"
DB_CONNECTION_URL="postgresql://postgres:postgresql@pps_postgresql:5432/pps"
DB_CONNECTION_POOL_MIN=1
DB_CONNECTION_POOL_MAX=5

# Overriding Apache Kafka section, as this project is dockerized
# For localhost run:
#KAFKA_BROKERS=localhost:9092 # for multiple brokers: KAFKA_BROKERS=localhost:9092,localhost:9093,localhost:9094
KAFKA_BROKERS=pps_kafka:19092 # for multiple brokers: KAFKA_BROKERS=localhost:9092,localhost:9093,localhost:9094
KAFKA_TOPIC_TRANSACTIONS=transactions

# Overriding Elasticsearch section, as this project is dockerized
# For localhost run:
#ELK_URL="http://localhost:9200" # URL to Elasticstack API, where logs will be send
ELK_URL="http://pps_elasticsearch:9200" # URL to Elasticstack API, where logs will be send
ELK_INDEX=pps-logs # Elastic Index name where all logs will be stored
